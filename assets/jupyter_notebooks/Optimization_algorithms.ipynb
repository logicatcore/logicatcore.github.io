{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization algorithms using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization algorithms\n",
    "Optimization algorithms play a central role in the learning process of most of the machine learning and deep learning methods. Here are some of the well known algorithms-\n",
    " 1. Vanilla Gradient descent\n",
    " 2. Gradient descent with Momentum\n",
    " 3. RMSprop\n",
    " 4. Adam\n",
    "\n",
    "While all the 4 above listed algorithms differ in their own way and have certain advantages and disadvantages. They share certain similarities with the simple graddient descent algorithm. In this blog post we will go through these 4 algorithms and see how they function on minimizing the loss or finding the minima of a random error function with multiple minimas and maximas.\n",
    "\n",
    "***\n",
    "\n",
    "## Error function with multiple minimas and maximas\n",
    "*Error Function* $= f(x,y) = 3 \\times e^{(-(y + 1)^2 - x^2)} \\times (x - 1)^2 - \\frac{e^{(-(x + 1)^2 - y^2)}}{3} + e^{(-x^2 - y^2)} \\times (10x^3 - 2x + 10y^5)$\n",
    "![Error functions](./Downloads/Figure_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: In this blog post, I will not be going into the theory of all the algorithms used rather just concentrate on the implementation and the results\n",
    "\n",
    "For theoretical reference please refer to [d2lai chapter on optimization algorithms](https://d2l.ai/chapter_optimization/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import torch\n",
    "import IPython\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from IPython import display\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "# mpl.rcParams['savefig.dpi'] = 300\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To draw 3d arrows in matplotlib\n",
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z(xx, yy)-> torch.tensor:\n",
    "    \"\"\"\n",
    "    Returns the loss at a certain point\n",
    "    \"\"\"\n",
    "    return 3 * torch.exp(-(yy + 1) ** 2 - xx ** 2) * (xx - 1) ** 2 - torch.exp(-(xx + 1) ** 2 - yy ** 2) / 3 + torch.exp(\n",
    "        -xx ** 2 - yy ** 2) * (10 * xx ** 3 - 2 * xx + 10 * yy ** 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 180 \n",
    "fps = 10     # frames per second - to save the progress in optimization as a video\n",
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=fps, metadata=dict(artist='Me'), bitrate=1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the plot with the error function terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4511eee900f4d7ab6124319d1fe758b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x7fbf3e11f820>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(-3, 3, 600)\n",
    "y = torch.linspace(-3, 3, 600)\n",
    "xgrid, ygrid = torch.meshgrid(x, y)\n",
    "zgrid = calc_z(xgrid, ygrid)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('Random Loss function: ' + '$f(x, y)$')\n",
    "ax.axis('auto')\n",
    "ax.plot_surface(xgrid.numpy(), ygrid.numpy(), zgrid.numpy(), cmap='viridis', alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Gradient Descent\n",
    "Gradient descent algorithm which is an iterative optimization algorithm can be described as loop which is executed repeatedly until certain convergence criteria has been met. Gradient descent can be explained using the following equation.\n",
    "\n",
    "### Gradient calculation\n",
    "$\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial (Error)}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error)}{\\partial y}\n",
    "\\end{vmatrix}$\n",
    "\n",
    "### Update equation\n",
    "$w_{x,y}^l = w_{x,y}^l - lr \\times \\frac{\\partial (Error)}{\\partial (w_{x,y}^l)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01                                                # learning rate\n",
    "\n",
    "xys = torch.tensor([-0.5, -0.7], requires_grad=True)     # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n",
    "\n",
    "new_z = 0\n",
    "dy_dx_current = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(i):\n",
    "    global dy_dx_current, xys, lr, new_z, ax\n",
    "    if i == 0:\n",
    "        # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n",
    "        xys = torch.tensor([-0.5, -0.7], requires_grad=True) \n",
    "        new_z = calc_z(xys[0], xys[1])\n",
    "        new_z.backward()\n",
    "        \n",
    "        dy_dx_current = xys.grad\n",
    "    \n",
    "    cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()]\n",
    "\n",
    "    xys = (xys - lr * dy_dx_current).clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # vanilla gradient descent\n",
    "    new_z = calc_z(xys[0], xys[1])\n",
    "    new_z.backward()\n",
    "    # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy)\n",
    "    dy_dx_current = xys.grad\n",
    "    \n",
    "    xys_plot = xys.detach().numpy()\n",
    "    ax.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker='s', c='r', s=50, zorder=3)\n",
    "    a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]],\n",
    "                [cache_pt[2], new_z.detach().numpy()], mutation_scale=5,\n",
    "                lw=2, arrowstyle=\"-|>\", color=\"r\")\n",
    "    ax.add_artist(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(fig, step, frames=epochs, interval=(1/fps)*1000, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML(anim.to_html5_video())\n",
    "anim.save('gd.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"gd.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent with momentum\n",
    "\n",
    "### Gradient calculation\n",
    "\n",
    "$\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial (Error)}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error)}{\\partial y}\n",
    "\\end{vmatrix} = \\beta * \\begin{vmatrix}\n",
    "\\frac{\\partial (Error)}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error)}{\\partial y}\n",
    "\\end{vmatrix} + (1 - \\beta) * \\begin{vmatrix}\n",
    "\\frac{\\partial (Error_{new})}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error_{new})}{\\partial y}\n",
    "\\end{vmatrix}$\n",
    "\n",
    "### Update equation\n",
    "$w_{x,y}^l = w_{x,y}^l - lr \\times \\frac{\\partial (Error)}{\\partial (w_{x,y}^l)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01                                                # learning rate\n",
    "\n",
    "xys = torch.tensor([-0.5, -0.7], requires_grad=True)     # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n",
    "\n",
    "new_z = 0\n",
    "dy_dx_current_gdm = 0\n",
    "\n",
    "dy_dx_new_gdm = torch.tensor([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gdm(i):\n",
    "    global dy_dx_new_gdm, dy_dx_current_gdm, xys, lr, new_z, ax\n",
    "    if i == 0:\n",
    "        # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n",
    "        xys = torch.tensor([-0.5, -0.7], requires_grad=True)\n",
    "        new_z = calc_z(xys[0], xys[1])\n",
    "        new_z.backward()\n",
    "        \n",
    "        dy_dx_current_gdm = xys.grad\n",
    "    \n",
    "    cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()]\n",
    "    \n",
    "    dy_dx_new_gdm = 0.9*dy_dx_new_gdm + (1 - 0.9)*dy_dx_current_gdm\n",
    "    xys = (xys - lr * dy_dx_new_gdm).clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # gradient descent with momentum\n",
    "    new_z = calc_z(xys[0], xys[1])\n",
    "    new_z.backward()\n",
    "    # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy)\n",
    "    dy_dx_current_gdm = xys.grad\n",
    "    \n",
    "    xys_plot = xys.detach().numpy()\n",
    "    ax.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker='s', c='g', s=50, zorder=3)\n",
    "    a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]],\n",
    "                [cache_pt[2], new_z.detach().numpy()], mutation_scale=5,\n",
    "                lw=2, arrowstyle=\"-|>\", color=\"g\")\n",
    "    ax.add_artist(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(fig, step_gdm, frames=epochs, interval=(1/fps)*1000, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML(anim.to_html5_video())\n",
    "anim.save('momentum.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"momentum.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    "\n",
    "### Gradient calculation\n",
    "$\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial (Error)}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error)}{\\partial y}\n",
    "\\end{vmatrix} = \\beta * \\begin{vmatrix}\n",
    "\\frac{\\partial (Error)}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error)}{\\partial y}\n",
    "\\end{vmatrix} + (1 - \\beta) * \\begin{vmatrix}\n",
    "\\frac{\\partial (Error_{new})}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error_{new})}{\\partial y}\n",
    "\\end{vmatrix}^2$\n",
    "\n",
    "### Update equation\n",
    "$w_{x,y}^l = w_{x,y}^l - lr \\times \\frac{\\frac{\\partial (Error_{new})}{\\partial (w_{x,y}^l)}}{\\sqrt{\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} + \\epsilon}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop_lr = 0.01                                           # learning rate\n",
    "\n",
    "xys = torch.tensor([-0.5, -0.7], requires_grad=True)        # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n",
    "\n",
    "epsilon = 1e-7                                              # small constant to avoid division by zero\n",
    "new_z = 0\n",
    "dy_dx_current_rmsprop = 0\n",
    "\n",
    "dy_dx_new_rmsprop = torch.tensor([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_rmsprop(i):\n",
    "    global dy_dx_new_rmsprop, dy_dx_current_rmsprop, xys, rmsprop_lr, new_z, ax\n",
    "    if i == 0:\n",
    "        # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n",
    "        xys = torch.tensor([-0.5, -0.7], requires_grad=True)\n",
    "        new_z = calc_z(xys[0], xys[1])\n",
    "        new_z.backward()\n",
    "\n",
    "        dy_dx_current_rmsprop = xys.grad\n",
    "    \n",
    "    cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()]\n",
    "    dy_dx_new_rmsprop = 0.9*dy_dx_new_rmsprop + (1 - 0.9)*torch.pow(dy_dx_current_rmsprop,2)\n",
    "    xys = (xys - rmsprop_lr * (dy_dx_current_rmsprop/(torch.sqrt(dy_dx_new_rmsprop) + epsilon))).clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # gradient descent with momentum\n",
    "    new_z = calc_z(xys[0], xys[1])\n",
    "    new_z.backward()\n",
    "    # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy)\n",
    "    dy_dx_current_rmsprop = xys.grad\n",
    "    \n",
    "    xys_plot = xys.detach().numpy()\n",
    "    ax.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker='s', c='b', s=50, zorder=3)\n",
    "    a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]],\n",
    "                [cache_pt[2], new_z.detach().numpy()], mutation_scale=5,\n",
    "                lw=2, arrowstyle=\"-|>\", color=\"b\")\n",
    "    ax.add_artist(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(fig, step_rmsprop, frames=epochs, interval=(1/fps)*1000, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML(anim.to_html5_video())\n",
    "anim.save('rmsprop.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"rmsprop.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "### Gradient calculation\n",
    "\n",
    "${\\partial (Error)}_{momentum} = \n",
    "\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial (Error)}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error)}{\\partial y}\n",
    "\\end{vmatrix} = \\beta_1 * \\begin{vmatrix}\n",
    "\\frac{\\partial (Error)}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error)}{\\partial y}\n",
    "\\end{vmatrix} + (1 - \\beta_1) * \\begin{vmatrix}\n",
    "\\frac{\\partial (Error_{new})}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error_{new})}{\\partial y}\n",
    "\\end{vmatrix}$\n",
    "\n",
    "${\\partial (Error)}_{rmsprop} = \n",
    "\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial (Error)}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error)}{\\partial y}\n",
    "\\end{vmatrix} = \\beta_2 * \\begin{vmatrix}\n",
    "\\frac{\\partial (Error)}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error)}{\\partial y}\n",
    "\\end{vmatrix} + (1 - \\beta_2) * \\begin{vmatrix}\n",
    "\\frac{\\partial (Error_{new})}{\\partial x} \\\\\n",
    "\\frac{\\partial (Error_{new})}{\\partial y}\n",
    "\\end{vmatrix}^2$\n",
    "\n",
    "### Update equation\n",
    "$w_{x,y}^l = w_{x,y}^l - lr \\times \\frac{\\partial (Error)_{momentum}}{\\sqrt{\\partial (Error)_{rmsprop} + \\epsilon}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_lr = 0.01                                           # learning rate\n",
    "\n",
    "xys = torch.tensor([-0.5, -0.7], requires_grad=True)     # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n",
    "\n",
    "epsilon = 1e-7                                           # small constant to avoid division by zero\n",
    "new_z = 0\n",
    "dy_dx_current_mom = 0\n",
    "dy_dx_current_rmsprop = 0\n",
    "\n",
    "dy_dx_new = torch.tensor([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_adam(i):\n",
    "    global dy_dx_current_mom, dy_dx_current_rmsprop, dy_dx_new, xys, adam_lr, new_z, ax\n",
    "    if i == 0:\n",
    "        # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n",
    "        xys = torch.tensor([-0.5, -0.7], requires_grad=True)\n",
    "        new_z = calc_z(xys[0], xys[1])\n",
    "        new_z.backward()\n",
    "\n",
    "        dy_dx_new = xys.grad\n",
    "    \n",
    "    cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()]\n",
    "    \n",
    "    dy_dx_current_mom = 0.9*dy_dx_current_mom + (1 - 0.9)*dy_dx_new\n",
    "    dy_dx_current_rmsprop = 0.9*dy_dx_current_rmsprop + (1 - 0.9)*torch.pow(dy_dx_new,2)\n",
    "    xys = (xys - adam_lr * (dy_dx_current_mom/(torch.sqrt(dy_dx_current_rmsprop) + epsilon))).clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # gradient descent with momentum\n",
    "    new_z = calc_z(xys[0], xys[1])\n",
    "    new_z.backward()\n",
    "    # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy)\n",
    "    dy_dx_new = xys.grad\n",
    "    \n",
    "    xys_plot = xys.detach().numpy()\n",
    "    ax.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker='s', c='c', s=50, zorder=3)\n",
    "    a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]],\n",
    "                [cache_pt[2], new_z.detach().numpy()], mutation_scale=5,\n",
    "                lw=2, arrowstyle=\"-|>\", color=\"c\")\n",
    "    ax.add_artist(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(fig, step_adam, frames=epochs, interval=(1/fps)*1000, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML(anim.to_html5_video())\n",
    "anim.save('adam.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"adam.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We see that all the algorithms find the minimas but take significatnly different paths. While Vanilla gradient descent and gradient descent with momentum find the minima faster compared to RMSprop and Adam, Other studies have proven Adam to be more stable and this tability allows to use higher learning rates as compared to the same learning rates used here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
